{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Finetune GPT-2 Text-Generating Model with Venture Capitalists Oral History Project (Berkeley). ",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SteliosKyriacou/gpt2-venture-capital-oral-history/blob/master/Finetune_GPT_2_Text_Generating_Model_with_Venture_Capitalists_Oral_History_Project_(Berkeley)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Train a GPT-2 Text-Generating Model w/ GPU For Free \n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: November 10th, 2019*\n",
        "\n",
        "Retrain an advanced text generating neural network on any text dataset **for free on a GPU using Collaboratory** using `gpt-2-simple`!\n",
        "\n",
        "For more about `gpt-2-simple`, you can visit [this GitHub repository](https://github.com/minimaxir/gpt-2-simple).\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes. (File -> Save a Copy in Drive)\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "outputId": "b682ff35-a467-46f8-ab42-1db99157515c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "!pip install -q gpt-2-simple\n",
        "import gpt_2_simple as gpt2\n",
        "from datetime import datetime\n",
        "from google.colab import files"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▌                               | 10kB 31.5MB/s eta 0:00:01\r\u001b[K     |█                               | 20kB 39.5MB/s eta 0:00:01\r\u001b[K     |█▌                              | 30kB 48.8MB/s eta 0:00:01\r\u001b[K     |██                              | 40kB 12.1MB/s eta 0:00:01\r\u001b[K     |██▌                             | 51kB 14.4MB/s eta 0:00:01\r\u001b[K     |███                             | 61kB 16.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 71kB 18.9MB/s eta 0:00:01\r\u001b[K     |████                            | 81kB 20.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 92kB 22.9MB/s eta 0:00:01\r\u001b[K     |█████                           | 102kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 112kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████                          | 122kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 133kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 143kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 153kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 163kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 174kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 184kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 194kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 204kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 215kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 225kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 235kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 245kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 256kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 266kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 276kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 286kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 296kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 307kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 317kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 327kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 337kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 348kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 358kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 368kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 378kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 389kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 399kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 409kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 419kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 430kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 440kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 450kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 460kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 471kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 481kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 491kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 501kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 512kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 522kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 532kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 542kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 552kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 563kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 573kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 583kB 24.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 593kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 604kB 24.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 614kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 624kB 24.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 634kB 24.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 645kB 24.6MB/s \n",
            "\u001b[?25h  Building wheel for gpt-2-simple (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bj2IJLHP3KwE",
        "colab_type": "text"
      },
      "source": [
        "## GPU\n",
        "\n",
        "Colaboratory uses either a Nvidia T4 GPU or an Nvidia K80 GPU. The T4 is slightly faster than the old K80 for training GPT-2, and has more memory allowing you to train the larger GPT-2 models and generate more text.\n",
        "\n",
        "You can verify which GPU is active by running the cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUmTooTW3osf",
        "colab_type": "code",
        "outputId": "f66f793e-6fc1-4308-a282-8b1910ca4c3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Thu Nov 14 09:02:31 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 430.50       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P8     9W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "## Downloading GPT-2\n",
        "\n",
        "If you're retraining a model on new text, you need to download the GPT-2 model first. \n",
        "\n",
        "There are three released sizes of GPT-2:\n",
        "\n",
        "* `124M` (default): the \"small\" model, 500MB on disk.\n",
        "* `355M`: the \"medium\" model, 1.5GB on disk.\n",
        "* `774M`: the \"large\" model, cannot currently be finetuned with Colaboratory but can be used to generate text from the pretrained model (see later in Notebook)\n",
        "* `1558M`: the \"extra large\", true model. Will not work if a K80 GPU is attached to the notebook. (like `774M`, it cannot be finetuned).\n",
        "\n",
        "Larger models have more knowledge, but take longer to finetune and longer to generate text. You can specify which base model to use by changing `model_name` in the cells below.\n",
        "\n",
        "The next cell downloads it from Google Cloud Storage and saves it in the Colaboratory VM at `/models/<model_name>`.\n",
        "\n",
        "This model isn't permanently saved in the Colaboratory VM; you'll have to redownload it if you want to retrain it at a later time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "outputId": "f4dd9f61-df6d-4cb3-afc5-8b0e54b0e4d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "gpt2.download_gpt2(model_name=\"124M\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fetching checkpoint: 1.05Mit [00:00, 391Mit/s]                                                      \n",
            "Fetching encoder.json: 1.05Mit [00:00, 167Mit/s]                                                    \n",
            "Fetching hparams.json: 1.05Mit [00:00, 384Mit/s]                                                    \n",
            "Fetching model.ckpt.data-00000-of-00001: 498Mit [00:01, 287Mit/s]                                   \n",
            "Fetching model.ckpt.index: 1.05Mit [00:00, 262Mit/s]                                                \n",
            "Fetching model.ckpt.meta: 1.05Mit [00:00, 137Mit/s]                                                 \n",
            "Fetching vocab.bpe: 1.05Mit [00:00, 154Mit/s]                                                       \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N8KXuKWzQSsN",
        "colab_type": "text"
      },
      "source": [
        "## Mounting Google Drive\n",
        "\n",
        "The best way to get input text to-be-trained into the Colaboratory VM, and to get the trained model *out* of Colaboratory, is to route it through Google Drive *first*.\n",
        "\n",
        "Running this cell (which will only work in Colaboratory) will mount your personal Google Drive in the VM, which later cells can use to get data in/out. (it will ask for an auth code; that auth is not saved anywhere)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "puq4iC6vUAHc",
        "colab_type": "code",
        "outputId": "de34309c-3f02-40ba-d94c-cfe7e799e672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "gpt2.mount_gdrive()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "## Uploading a Text File to be Trained to Colaboratory\n",
        "\n",
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any smaller text file**  (<10 MB) and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"MosheAlafi.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HeeSKtNWUedE",
        "colab_type": "text"
      },
      "source": [
        "If your text file is larger than 10MB, it is recommended to upload that file to Google Drive first, then copy that file from Google Drive to the Colaboratory VM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z6okFD8VKtS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_file_from_gdrive(file_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "## Finetune GPT-2\n",
        "\n",
        "The next cell will start the actual finetuning of GPT-2. It creates a persistent TensorFlow session which stores the training config, then runs the training for the specified number of `steps`. (to have the finetuning run indefinitely, set `steps = -1`)\n",
        "\n",
        "The model checkpoints will be saved in `/checkpoint/run1` by default. The checkpoints are saved every 500 steps (can be changed) and when the cell is stopped.\n",
        "\n",
        "The training might time out after 4ish hours; make sure you end training and save the results so you don't lose them!\n",
        "\n",
        "**IMPORTANT NOTE:** If you want to rerun this cell, **restart the VM first** (Runtime -> Restart Runtime). You will need to rerun imports but not recopy files.\n",
        "\n",
        "Other optional-but-helpful parameters for `gpt2.finetune`:\n",
        "\n",
        "\n",
        "*  **`restore_from`**: Set to `fresh` to start training from the base GPT-2, or set to `latest` to restart training from an existing checkpoint.\n",
        "* **`sample_every`**: Number of steps to print example output\n",
        "* **`print_every`**: Number of steps to print training progress.\n",
        "* **`learning_rate`**:  Learning rate for the training. (default `1e-4`, can lower to `1e-5` if you have <1MB input data)\n",
        "*  **`run_name`**: subfolder within `checkpoint` to save the model. This is useful if you want to work with multiple models (will also need to specify  `run_name` when loading the model)\n",
        "* **`overwrite`**: Set to `True` if you want to continue finetuning an existing model (w/ `restore_from='latest'`) without creating duplicate copies. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "cd67fae0-0f41-46e2-b21c-3f365cc404af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "sess = gpt2.start_tf_sess()\n",
        "\n",
        "gpt2.finetune(sess,\n",
        "              dataset=file_name,\n",
        "              model_name='124M',\n",
        "              steps=1000,\n",
        "              restore_from='fresh',\n",
        "              run_name='alafi1',\n",
        "              print_every=10,\n",
        "              sample_every=200,\n",
        "              save_every=500\n",
        "              )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/gpt_2_simple/src/sample.py:17: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Loading checkpoint models/124M/model.ckpt\n",
            "INFO:tensorflow:Restoring parameters from models/124M/model.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/1 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loading dataset...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 1/1 [00:00<00:00,  3.39it/s]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "dataset has 50077 tokens\n",
            "Training...\n",
            "[10 | 27.62] loss=2.65 avg=2.65\n",
            "[20 | 49.70] loss=2.36 avg=2.50\n",
            "[30 | 72.36] loss=2.19 avg=2.40\n",
            "[40 | 95.92] loss=1.99 avg=2.29\n",
            "[50 | 119.97] loss=1.79 avg=2.19\n",
            "[60 | 143.34] loss=1.61 avg=2.09\n",
            "[70 | 166.61] loss=1.69 avg=2.03\n",
            "[80 | 190.11] loss=1.50 avg=1.96\n",
            "[90 | 213.73] loss=1.17 avg=1.87\n",
            "[100 | 237.21] loss=1.24 avg=1.81\n",
            "[110 | 260.69] loss=0.68 avg=1.70\n",
            "[120 | 284.16] loss=0.63 avg=1.61\n",
            "[130 | 307.66] loss=0.49 avg=1.51\n",
            "[140 | 331.15] loss=0.27 avg=1.42\n",
            "[150 | 354.65] loss=0.50 avg=1.35\n",
            "[160 | 378.17] loss=0.23 avg=1.28\n",
            "[170 | 401.68] loss=0.18 avg=1.21\n",
            "[180 | 425.19] loss=0.15 avg=1.14\n",
            "[190 | 448.68] loss=0.08 avg=1.08\n",
            "[200 | 472.17] loss=0.12 avg=1.03\n",
            "======== SAMPLE 1 ========\n",
            " building. And you have to have a feeling what they\n",
            "really thought they were doing. Also, a lot of Genentech got smart investors like Carter and\n",
            "Weiss. They invested a lot of money in Genentech, and the investment bankers at\n",
            "Genentech were really head and shoulders above the people.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "They were not the professionals you think they were?\n",
            "Alafi:\n",
            "No. They were professionals, and they were chosen because they have good names, like [Ronald]\n",
            "Goldacre, [Joseph] Lundmann, all of them. But they really were going to do something\n",
            "behind the scenes, and in this company, they really had it figured. At some point, Jim Vincent\n",
            "attached the chair to the mat, and immediately he started thinking big. He really thought, this is\n",
            "what this should be. And within the first year, he doesn’t know what to do with it. He\n",
            "didn’t know what to do with it. Then he got really big, and now he is trying to\n",
            "do it because he is driving him crazy. He went to his cousin in Chicago, and he still doesn’t know how to drive.\n",
            "So he was trying to find a place in the city, and the way to find a place is to get to certain points on the map.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "How are these people doing for them what they were trying to find in their cousins’s way?\n",
            "Alafi:\n",
            "We were trying to find a better position than Cetus to be more involved with Biogen, and therefore\n",
            "more involved in the decision-making, in the decision-making process. And they found a place.\n",
            "Neil [Weiss] was doing interferon in his lab, and we thought we would get it by\n",
            "solving a problem in situ. But really, it involved more than just sticking to the manufacturer guidelines,\n",
            "and trying to get the gene into a T cells study using interferon. Just as a matter of fact, when we\n",
            "started putting money in just a few hundred thousand, just like they started putting money in just a few\n",
            "thousand, they all changed departments. Just the scientists. The whole department. It\n",
            "didn’t take long at Biogen, actually. Terry Swanson came from Biogen, and he came home\n",
            "okay. He was shocked and sad because his colleague died. They didn’t know what to do.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "Swanson?\n",
            "Alafi:\n",
            "No. He was a Renaissance man. He came from Spain. He studied at the university. Then came Donatello, who is from\n",
            "Rocca, came from Cetus. They had a meeting in Geneva, and they made a decision. They said, “This\n",
            "is a major project,” and that’s when we\n",
            "really had the decision made.\n",
            "Ultimately, Lederberg changed his mind. He said later, “This is a major thing,” and\n",
            "then he said it. Lederberg:\n",
            "Yes. The president said that’s Lederberg’s idea. He said to him, “How much does it cost?” Later, at\n",
            "Genentech, he was going to have to spend something. And after that, T cells are expensive. I got Wally\n",
            "Turk to give me hundred fifty million each. I am trying to remember. One day, I\n",
            "said to Kleiner Perkins that yes,’s I was going to have to spend tens of thousands of\n",
            "dollar, three hundred million, in competition. They said, “Okay, you come with us.” Kleiner Perkins has\n",
            "been friends with Cetus for a long time. I said, “Okay, we will give you a hundred thousand\n",
            "dollars.” They accepted. We started working on the insulin pump and the microbial\n",
            "screening, and all of that. Then we began doing clinical trials. But before that, there was\n",
            "never any company who was going to offer a hundred thousand—yes, I understood it.\n",
            "We were working capital. We were finishing the Barys paper. The guy at UCLA said,\n",
            "“No, we are not interested.” For a hundred thousand dollars, he has to develop it. The rest of\n",
            "it.” Another guy, the one who came with Cetus, said, “How do you do it?” We had\n",
            "an attorney from Inco who came who started all of these companies. He is from Inco.\n",
            "And after they developed products, they sell it to the world. So we were working\n",
            "capital. Yes? Then we started working on the “Blade Runner’--the first movie. The same group\n",
            "\n",
            "[210 | 507.57] loss=0.10 avg=0.98\n",
            "[220 | 531.07] loss=0.11 avg=0.94\n",
            "[230 | 554.59] loss=0.09 avg=0.90\n",
            "[240 | 578.08] loss=0.07 avg=0.86\n",
            "[250 | 601.57] loss=0.07 avg=0.82\n",
            "[260 | 625.07] loss=0.08 avg=0.79\n",
            "[270 | 648.57] loss=0.06 avg=0.76\n",
            "[280 | 672.06] loss=0.04 avg=0.73\n",
            "[290 | 695.57] loss=0.06 avg=0.70\n",
            "[300 | 719.06] loss=0.04 avg=0.68\n",
            "[310 | 742.57] loss=0.05 avg=0.65\n",
            "[320 | 766.06] loss=0.05 avg=0.63\n",
            "[330 | 789.56] loss=0.06 avg=0.61\n",
            "[340 | 813.04] loss=0.04 avg=0.59\n",
            "[350 | 836.47] loss=0.06 avg=0.57\n",
            "[360 | 859.91] loss=0.04 avg=0.56\n",
            "[370 | 883.34] loss=0.05 avg=0.54\n",
            "[380 | 906.78] loss=0.05 avg=0.53\n",
            "[390 | 930.27] loss=0.05 avg=0.51\n",
            "[400 | 953.74] loss=0.04 avg=0.50\n",
            "======== SAMPLE 1 ========\n",
            " on a\n",
            "record, I felt, because I am Canadian, I want to say something positive about Canada. I\n",
            "think they are doing very well, but I also think they are not doing as they should.\n",
            "As I was telling you, the board voted in June to begin business operations in Canada.\n",
            "Of course, the first year was a disaster. I even went as far as saying that\n",
            "the Canadian dollar is not the currency of the world. The Europeans will pay you if you sell their\n",
            "cars. The other thing is, when you sell them, you get a big inheritance from your father. So if\n",
            "they sell them, they have a big part of the business.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "Did you know?\n",
            "Alafi:\n",
            "I knew of that one or two friends who did deals, and the inheritance was big, so it was a\n",
            "big deal. We started going to certain companies and telling them the dates, the market value of\n",
            "the products, and how much is it in the pipeline. And they said, “No, it doesn’t\n",
            "matter, it’s four hundred fifty.” Then we were going to have a board meeting in Paris for\n",
            "this project in the States. And I remember the first thing he said was, “Ugh, that’s expensive.” I said, “You’ve got\n",
            "($1 million)\n",
            "money ready; I want to spend it.” He said, “But how much do you want?” I said, “Now,\n",
            "let’s see.” An hour, we’ll start a company. So we sat with Kevin Systrom, and he went with me. I\n",
            "came with Sam Eleter from Cetus. We knew about the investment because we had seen in\n",
            "Cetus a lot of companies where they put money in people before they even put a penny.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "You mentioned that certain requirements of venture capital might be a little bit different than most start-ups?\n",
            "Alafi:\n",
            "I really am the one who put money in her: I know him and my wife Separation Act investors. I gave her\n",
            "money and started companies. Then we went and invested in different companies, and then we ended up in Sequoia.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "It sounds like a lot for two people.\n",
            "Alafi:\n",
            "A lot, yes. But when we put money in other companies, yes? The same thing with their own management, yes?\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "You talked about Murray Hill last time, but you didn’t give it a name.\n",
            "Alafi:\n",
            "Yes, I had a limited partnership with Carter’s money, and I started companies with it. Then we\n",
            "became very close friends with Hardoon in Chicago, with his idea of “A’m not based in\n",
            "China, but in the Middle East and North Africa. So that was the beginning.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "The name made some people think it’s some sort of a madman mode of investing.\n",
            "Alafi:\n",
            "It was not so much a mode of investing in technology and high-yield stocks, than in the potential of\n",
            "the technology in the long run, to find new things to do.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "Which was more important to him than anything else?\n",
            "Alafi:\n",
            "If it is the technology, I value it. If it can be found in a lab, I value it.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "The potential of the technology?\n",
            "Alafi:\n",
            "Yes. I still remember when I went to visit my father, and I said, “I love science! You\n",
            "should invest in biology or something. The technology is there, you should just take it\n",
            "for yourself.” My father was a Nobel Prize winner. I still think that is his belief.\n",
            "But in the intervening years, my father, and my friends started making money in oil and gas\n",
            "development. They would send people to Baghdad and train the kids there to start companies. That\n",
            "was the beginning of Afro-Caribbean business ideas. You could invest in everything from chemical\n",
            "oil to solar cells. The whole concept was new. The Arabs did not need banks, they could\n",
            "go their own way. They could do it for themselves.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "What did they think you were going to do? What did you think you were going to do?\n",
            "Alafi:\n",
            "I was going to start a company for the application of radiation to produce petri dishes. I don’t know\n",
            "\n",
            "[410 | 987.90] loss=0.04 avg=0.48\n",
            "[420 | 1011.34] loss=0.04 avg=0.47\n",
            "[430 | 1034.79] loss=0.03 avg=0.46\n",
            "[440 | 1058.27] loss=0.04 avg=0.45\n",
            "[450 | 1081.77] loss=0.03 avg=0.43\n",
            "[460 | 1105.26] loss=0.05 avg=0.42\n",
            "[470 | 1128.76] loss=0.03 avg=0.41\n",
            "[480 | 1152.26] loss=0.04 avg=0.40\n",
            "[490 | 1175.73] loss=0.03 avg=0.39\n",
            "[500 | 1199.19] loss=0.03 avg=0.39\n",
            "Saving checkpoint/alafi1/model-500\n",
            "[510 | 1225.26] loss=0.05 avg=0.38\n",
            "[520 | 1248.78] loss=0.04 avg=0.37\n",
            "[530 | 1272.24] loss=0.03 avg=0.36\n",
            "[540 | 1295.67] loss=0.03 avg=0.35\n",
            "[550 | 1319.11] loss=0.05 avg=0.35\n",
            "[560 | 1342.57] loss=0.04 avg=0.34\n",
            "[570 | 1366.03] loss=0.03 avg=0.33\n",
            "[580 | 1389.53] loss=0.03 avg=0.32\n",
            "[590 | 1413.01] loss=0.03 avg=0.32\n",
            "[600 | 1436.46] loss=0.03 avg=0.31\n",
            "======== SAMPLE 1 ========\n",
            "\n",
            "in the next three or four years, until we had a president\n",
            "[James] Golf’s son was born, to raise money for the bank.\n",
            "But we didn’t need a president anymore. We had enough attorneys, just like\n",
            "we had enough physicians. And I think even after the war ended, the Jews had rifles in the streets.\n",
            "They killed all the Jews in the ghetto, and they went to India to fight. The Mufti in Delhi brought them back\n",
            "with him. For those Jews, it was like a vamp, they were controlling all the news, all the\n",
            "things that was coming out of the Middle East. It was like a vengeful Hanukkah.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "And the war was over for good.\n",
            "Alafi:\n",
            " for the war.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "Did you leave the military?\n",
            "Alafi:\n",
            "Yes.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "Yes.\n",
            "Alafi:\n",
            "I left when I was 13. I remember I went to Israel with my father, and I said, “I have these men, and they\n",
            "were making statistics. They are making the available data, and they knew it.”\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "They knew what they were doing, right?\n",
            "Alafi:\n",
            "They knew what the hell they were doing, because when we started building\n",
            "infrastructure, Chevron started making money. We knew, that’s not what we were\n",
            "doing. That’s marketing. I left because that’s not what we were doing.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "How did you leave?\n",
            "Alafi:\n",
            "By my father’s agreement, they started giving me years, until I was about twenty-three, before I was allowed to apply for\n",
            "a patent. And I had them work on my behalf. And once a patent is granted, that’s your\n",
            "null card.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "Well, in your case it’s not nullification, it’s as if you and your technology were married?\n",
            "Alafi:\n",
            "Yes, and that was the beginning.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "All kinds of technologies are being designed that could one day control our cancers.\n",
            "Alafi:\n",
            "Yes. We were thinking, What’s his technology doing here? What does his technology do? And we said to\n",
            "them, “We are going to develop it. Then we’ll do anything\n",
            "we want with it.” They said, “Let’s look at this bomb and see.”\n",
            "We’ll supply a talking point. [Approaching a chart on a table] we can sell it to others forlornly. Then\n",
            "we say, “That’s great.” And we start thinking we’d be the first to say the atomic—plus we have time,\n",
            "we can do something with it. [Approaching a chart on a table] we can sell it to others forlornly. Then we\n",
            "put a lot more money in the beginning, and we’ll do it faster. I am telling you, Apple engineers\n",
            "have said, “It doesn’t make sense. We can’t do it like this.” How can we trust them?\n",
            "I’ll show you how. First we sold our first million-plus thousand thousand copies. Then we sold it\n",
            "to Cancer Research International. Bill Gates came and said he wants to raise a hundred million\n",
            "dollars. And I said, “How much do we need to raise to make this investment?” He said, “So, what do we\n",
            "have? Two thousand and a half thousand.” I said, “The idea is to000 the rate of reaction in\n",
            "cancer drugs.” He said, “The other thing, you would have to be very, very great to get there.”\n",
            "I have a friend, Donny Strasbourg, from the Pasteur [Institute]. He has a number of cancer\n",
            "research groups all over France. They want to know how many mice are in a certain area.\n",
            "They have a dinner on the outskirts of Paris on June 21. Robin Hooded in that area is the theme.\n",
            "Biogen is going to be one of those groups. They will sell the rights to it to Novartis. And\n",
            "Novartis seems to be doing a great job. But the company also has a history. In the sixties, schwarze 1 was running\n",
            "in the Germans. They had a whole separate building, half a world\n",
            "\n",
            "[610 | 1470.83] loss=0.03 avg=0.31\n",
            "[620 | 1494.23] loss=0.03 avg=0.30\n",
            "[630 | 1517.71] loss=0.04 avg=0.29\n",
            "[640 | 1541.21] loss=0.03 avg=0.29\n",
            "[650 | 1564.72] loss=0.03 avg=0.28\n",
            "[660 | 1588.22] loss=0.04 avg=0.28\n",
            "[670 | 1611.73] loss=0.03 avg=0.27\n",
            "[680 | 1635.23] loss=0.02 avg=0.27\n",
            "[690 | 1658.73] loss=0.03 avg=0.26\n",
            "[700 | 1682.23] loss=0.04 avg=0.26\n",
            "[710 | 1705.68] loss=0.03 avg=0.25\n",
            "[720 | 1729.10] loss=0.03 avg=0.25\n",
            "[730 | 1752.53] loss=0.03 avg=0.25\n",
            "[740 | 1775.96] loss=0.03 avg=0.24\n",
            "[750 | 1799.42] loss=0.02 avg=0.24\n",
            "[760 | 1822.82] loss=0.03 avg=0.23\n",
            "[770 | 1846.30] loss=0.04 avg=0.23\n",
            "[780 | 1869.79] loss=0.03 avg=0.23\n",
            "[790 | 1893.22] loss=0.03 avg=0.22\n",
            "[800 | 1916.71] loss=0.02 avg=0.22\n",
            "======== SAMPLE 1 ========\n",
            " so I went on vacation for a month.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "In November?\n",
            "Alafi:\n",
            "Not too bad. Then I stayed one day in Geneva, thinking, You visited my cousin in Iraq. Tell him I had\n",
            "good news. He wouldn’t believe it! He even looked at me. He said to myself, “How come?”\n",
            "[laughter] I quickly got up and started going\n",
            "north, starting with the most recent round of agreements I had with the pharmaceutical\n",
            "companies. The agreements that had been in the works for some time didn’t materialize.\n",
            "Some of them did, but nothing happened to them. All of them wereawed. Schering cut the\n",
            "most expensive part of the interferon, and then most of the nonrecombinant stuff, without any\n",
            "selection bias.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "Tell me about Qiagen, another company you founded.66\n",
            "Alafi:\n",
            "I said I would meet with one of Charles Weissmann’s postdocs. I met him and another guy\n",
            "[Metin Coltan? and Karstem Henco?] at the airport in Frankfurt. I was so impressed with them. I\n",
            "said to myself, even if the concept didn’t work, these scientists are smart enough to know how to\n",
            "change direction. And they raised the money, and they almost lost all the money. They were able\n",
            "to raise a little bit more from the insiders. But the banks are tough in Germany; they don’t give\n",
            "you money. When they wanted to raise more money, the board—I wasn’t on the board—had\n",
            "them go to the bank, and put their homes as collateral before they would give them money. No\n",
            "scientists in the U.S. will ever accept that. But ultimately, they really developed something\n",
            "substantial. Enter any lab, you see Qiagen products for DNA purification.\n",
            "[tape interruption]\n",
            "They were two young people; they had never run a company. I would start anything with them.\n",
            "That’s how good they were without being from a business school like Harvard or Stanford.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "Oxford GlycoSciences?\n",
            "Alafi:\n",
            "Monsanto was interested because they were funding Raymond Dwerk, who was in glycobiology\n",
            "at Oxford. Through his technology, Howard Schneiderman said, “Why don’t you start a\n",
            "company with Oxford technology?” I went and started a company with a British venture capital\n",
            "firm. Over the years, they built sequencers for sugar, carbohydrates. But it never took off. Then\n",
            "they changed and went into developing drugs, and Kirk Raab from Genentech took over as CEO.\n",
            "They thought it could be just as big in supporting sequencing. It didn’t materialize. Then they\n",
            "were raising money.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "Why?\n",
            "Alafi:\n",
            "They were scared of them. I was with them, and they spent the first half of\n",
            "the seventies and fifties, fighting over who could invest in the company. And the board became more and\n",
            "more conservative, and the management became more andmore convinced that Genentech was the one that\n",
            "should get the Nobel Prize.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "Because of the war?\n",
            "Alafi:\n",
            "Yes, it’s a good question. As the war wore on, the world changed. The pharmaceutical industry\n",
            "was in its infancy. The Jews all over the world were consuming alcoholic beverages. I\n",
            "remember hearing from my father [Joseph Alafi] that the first thing they would drink when they went to school was\n",
            "Francis Appleton’s. Once they got outside, they would have none of the alcohol.\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "The Jews were also Americans—not the least of which was Nixon. At the beginning of\n",
            "the revolution, the British colonial government was in agreement with the Americans to\n",
            "take over running the country. Now,—————————they didn’t know what to do with it.\n",
            "[tape interruption]\n",
            "It was against the grain at that time, really. I remember the prime minister said to\n",
            "the British, “Why can’t we find a company in Europe that wants to take over?” And the Europeans said, “\n",
            "\n",
            "“Okay, you are really off the hook.” And the Europeans said, “How about Europe?” And the Americans\n",
            "said, “We want to take over and do something for the company, not for the company.”\n",
            "<|endoftext|>\n",
            "Interviewer:\n",
            "It was a major\n",
            "\n",
            "[810 | 1951.03] loss=0.04 avg=0.22\n",
            "[820 | 1974.43] loss=0.03 avg=0.21\n",
            "[830 | 1997.87] loss=0.04 avg=0.21\n",
            "[840 | 2021.30] loss=0.03 avg=0.21\n",
            "[850 | 2044.78] loss=0.03 avg=0.20\n",
            "[860 | 2068.25] loss=0.04 avg=0.20\n",
            "[870 | 2091.73] loss=0.03 avg=0.20\n",
            "[880 | 2115.17] loss=0.03 avg=0.19\n",
            "[890 | 2138.63] loss=0.03 avg=0.19\n",
            "[900 | 2162.11] loss=0.03 avg=0.19\n",
            "[910 | 2185.59] loss=0.03 avg=0.19\n",
            "[920 | 2209.07] loss=0.02 avg=0.18\n",
            "[930 | 2232.57] loss=0.03 avg=0.18\n",
            "[940 | 2256.07] loss=0.04 avg=0.18\n",
            "[950 | 2279.56] loss=0.03 avg=0.18\n",
            "[960 | 2303.07] loss=0.02 avg=0.17\n",
            "[970 | 2326.57] loss=0.03 avg=0.17\n",
            "[980 | 2350.03] loss=0.03 avg=0.17\n",
            "[990 | 2373.53] loss=0.02 avg=0.17\n",
            "[1000 | 2397.01] loss=0.04 avg=0.16\n",
            "Saving checkpoint/alafi1/model-1000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IXSuTNERaw6K",
        "colab_type": "text"
      },
      "source": [
        "After the model is trained, you can copy the checkpoint folder to your own Google Drive.\n",
        "\n",
        "If you want to download it to your personal computer, it's strongly recommended you copy it there first, then download from Google Drive. The checkpoint folder is copied as a `.rar` compressed file; you can download it and uncompress it locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHdTL8NDbAh3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gpt2.copy_checkpoint_to_gdrive(run_name='alafi1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQJgV_b4bmzd",
        "colab_type": "text"
      },
      "source": [
        "You're done! Feel free to go to the **Generate Text From The Trained Model** section to generate text based on your retrained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dN5K1ElkkKlV"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dpm2r1lJkKlJ"
      },
      "source": [
        "# LICENSE\n",
        "\n",
        "MIT License\n",
        "\n",
        "Copyright (c) 2019 Max Woolf\n",
        "\n",
        "Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "of this software and associated documentation files (the \"Software\"), to deal\n",
        "in the Software without restriction, including without limitation the rights\n",
        "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "copies of the Software, and to permit persons to whom the Software is\n",
        "furnished to do so, subject to the following conditions:\n",
        "\n",
        "The above copyright notice and this permission notice shall be included in all\n",
        "copies or substantial portions of the Software.\n",
        "\n",
        "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n",
        "SOFTWARE."
      ]
    }
  ]
}